{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern Analysis for Hierarchical Rule Generation\n",
    "\n",
    "This notebook analyzes the DNS training dataset to extract discriminative patterns for building a hierarchical classifier.\n",
    "\n",
    "## Goals:\n",
    "1. Analyze TLD distribution between legitimate and malicious domains\n",
    "2. Extract domain name patterns (length, special characters, entropy)\n",
    "3. Identify subdomain characteristics\n",
    "4. Calculate information gain for each feature\n",
    "5. Generate candidate rules ordered by performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('../../Data/dns_training_data.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "print(f\"\\nMalicious ratio: {df['label'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. TLD Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tld(domain):\n",
    "    \"\"\"Extract top-level domain\"\"\"\n",
    "    parts = domain.split('.')\n",
    "    if len(parts) >= 2:\n",
    "        return parts[-1]\n",
    "    return parts[0] if parts else ''\n",
    "\n",
    "df['tld'] = df['domain'].apply(extract_tld)\n",
    "\n",
    "# TLD distribution by class\n",
    "tld_stats = df.groupby('tld').agg({\n",
    "    'label': ['count', 'sum', 'mean']\n",
    "}).round(3)\n",
    "tld_stats.columns = ['total_count', 'malicious_count', 'malicious_ratio']\n",
    "tld_stats = tld_stats.sort_values('total_count', ascending=False)\n",
    "\n",
    "print(\"Top 20 TLDs:\")\n",
    "print(tld_stats.head(20))\n",
    "\n",
    "# High-risk TLDs (>70% malicious, min 10 samples)\n",
    "high_risk_tlds = tld_stats[\n",
    "    (tld_stats['malicious_ratio'] > 0.7) & (tld_stats['total_count'] >= 10)\n",
    "].sort_values('malicious_ratio', ascending=False)\n",
    "\n",
    "print(f\"\\nHigh-risk TLDs (>70% malicious, n>=10): {len(high_risk_tlds)}\")\n",
    "print(high_risk_tlds.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Domain Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_domain_features(domain):\n",
    "    \"\"\"Extract features from domain name\"\"\"\n",
    "    parts = domain.split('.')\n",
    "    \n",
    "    # Get main domain (second-level domain)\n",
    "    if len(parts) >= 2:\n",
    "        main_domain = parts[-2]\n",
    "    else:\n",
    "        main_domain = parts[0] if parts else ''\n",
    "    \n",
    "    features = {\n",
    "        'domain_length': len(domain),\n",
    "        'main_domain_length': len(main_domain),\n",
    "        'num_subdomains': len(parts) - 2 if len(parts) > 2 else 0,\n",
    "        'num_dots': domain.count('.'),\n",
    "        'num_hyphens': domain.count('-'),\n",
    "        'num_digits': sum(c.isdigit() for c in domain),\n",
    "        'has_www': 1 if domain.startswith('www.') else 0,\n",
    "        'digit_ratio': sum(c.isdigit() for c in main_domain) / len(main_domain) if main_domain else 0,\n",
    "    }\n",
    "    \n",
    "    # Calculate entropy\n",
    "    if main_domain:\n",
    "        char_freq = Counter(main_domain)\n",
    "        entropy = -sum((freq/len(main_domain)) * np.log2(freq/len(main_domain)) \n",
    "                      for freq in char_freq.values())\n",
    "        features['entropy'] = entropy\n",
    "    else:\n",
    "        features['entropy'] = 0\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Extract features for all domains\n",
    "feature_df = pd.DataFrame([extract_domain_features(d) for d in df['domain']])\n",
    "df_features = pd.concat([df, feature_df], axis=1)\n",
    "\n",
    "print(\"Feature statistics by class:\")\n",
    "print(df_features.groupby('label')[feature_df.columns].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Information Gain Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Calculate information gain for numerical features\n",
    "def calculate_information_gain(df, feature_col, target_col='label', bins=10):\n",
    "    \"\"\"Calculate information gain for a feature\"\"\"\n",
    "    # Discretize continuous features\n",
    "    feature_binned = pd.cut(df[feature_col], bins=bins, duplicates='drop')\n",
    "    return mutual_info_score(df[target_col], feature_binned)\n",
    "\n",
    "# Calculate IG for all numerical features\n",
    "ig_scores = {}\n",
    "for col in feature_df.columns:\n",
    "    ig_scores[col] = calculate_information_gain(df_features, col)\n",
    "\n",
    "ig_df = pd.DataFrame([\n",
    "    {'feature': k, 'information_gain': v} \n",
    "    for k, v in ig_scores.items()\n",
    "]).sort_values('information_gain', ascending=False)\n",
    "\n",
    "print(\"\\nInformation Gain for features:\")\n",
    "print(ig_df)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(ig_df['feature'], ig_df['information_gain'])\n",
    "plt.xlabel('Information Gain')\n",
    "plt.title('Feature Importance by Information Gain')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Decision Tree for Rule Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Prepare features\n",
    "X = df_features[feature_df.columns]\n",
    "y = df_features['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train decision tree (this will help us extract rules)\n",
    "dt = DecisionTreeClassifier(\n",
    "    max_depth=10,\n",
    "    min_samples_split=50,\n",
    "    min_samples_leaf=20,\n",
    "    random_state=42\n",
    ")\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = dt.predict(X_test)\n",
    "print(\"\\nDecision Tree Performance:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Legitimate', 'Malicious']))\n",
    "\n",
    "# Extract rules\n",
    "tree_rules = export_text(dt, feature_names=list(feature_df.columns))\n",
    "print(\"\\nExtracted Rules (first 50 lines):\")\n",
    "print('\\n'.join(tree_rules.split('\\n')[:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Hierarchical Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract simple rules with performance metrics\n",
    "def generate_candidate_rules(df_features, tld_stats):\n",
    "    \"\"\"Generate candidate rules with performance metrics\"\"\"\n",
    "    rules = []\n",
    "    \n",
    "    # Rule 1: High-risk TLDs\n",
    "    high_risk = tld_stats[\n",
    "        (tld_stats['malicious_ratio'] > 0.7) & (tld_stats['total_count'] >= 10)\n",
    "    ]\n",
    "    for tld in high_risk.index:\n",
    "        mask = df_features['tld'] == tld\n",
    "        if mask.sum() > 0:\n",
    "            precision = df_features[mask]['label'].mean()\n",
    "            coverage = mask.sum() / len(df_features)\n",
    "            rules.append({\n",
    "                'type': 'tld',\n",
    "                'condition': f\"TLD == '{tld}'\",\n",
    "                'prediction': 1,\n",
    "                'precision': precision,\n",
    "                'coverage': coverage,\n",
    "                'score': precision * coverage\n",
    "            })\n",
    "    \n",
    "    # Rule 2: Domain length based rules\n",
    "    for threshold in [10, 15, 20, 30, 40]:\n",
    "        mask = df_features['domain_length'] > threshold\n",
    "        if mask.sum() > 0:\n",
    "            precision = df_features[mask]['label'].mean()\n",
    "            coverage = mask.sum() / len(df_features)\n",
    "            if precision > 0.6:  # Only keep if reasonably predictive\n",
    "                rules.append({\n",
    "                    'type': 'domain_length',\n",
    "                    'condition': f'domain_length > {threshold}',\n",
    "                    'prediction': 1,\n",
    "                    'precision': precision,\n",
    "                    'coverage': coverage,\n",
    "                    'score': precision * coverage\n",
    "                })\n",
    "    \n",
    "    # Rule 3: Number of subdomains\n",
    "    for threshold in [2, 3, 4]:\n",
    "        mask = df_features['num_subdomains'] >= threshold\n",
    "        if mask.sum() > 0:\n",
    "            precision = df_features[mask]['label'].mean()\n",
    "            coverage = mask.sum() / len(df_features)\n",
    "            if precision > 0.6:\n",
    "                rules.append({\n",
    "                    'type': 'subdomain',\n",
    "                    'condition': f'num_subdomains >= {threshold}',\n",
    "                    'prediction': 1,\n",
    "                    'precision': precision,\n",
    "                    'coverage': coverage,\n",
    "                    'score': precision * coverage\n",
    "                })\n",
    "    \n",
    "    # Rule 4: Digit ratio\n",
    "    for threshold in [0.2, 0.3, 0.4, 0.5]:\n",
    "        mask = df_features['digit_ratio'] > threshold\n",
    "        if mask.sum() > 0:\n",
    "            precision = df_features[mask]['label'].mean()\n",
    "            coverage = mask.sum() / len(df_features)\n",
    "            if precision > 0.6:\n",
    "                rules.append({\n",
    "                    'type': 'digit_ratio',\n",
    "                    'condition': f'digit_ratio > {threshold}',\n",
    "                    'prediction': 1,\n",
    "                    'precision': precision,\n",
    "                    'coverage': coverage,\n",
    "                    'score': precision * coverage\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(rules).sort_values('score', ascending=False)\n",
    "\n",
    "rules_df = generate_candidate_rules(df_features, tld_stats)\n",
    "print(f\"\\nGenerated {len(rules_df)} candidate rules\")\n",
    "print(\"\\nTop 20 rules by score:\")\n",
    "print(rules_df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save extracted patterns\n",
    "tld_stats.to_csv('../data/tld_analysis.csv')\n",
    "rules_df.to_csv('../data/candidate_rules.csv', index=False)\n",
    "ig_df.to_csv('../data/feature_importance.csv', index=False)\n",
    "\n",
    "print(\"Analysis results saved to data/ directory\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
